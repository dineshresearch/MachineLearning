Multiple comparisons
In statistics, the multiple comparisons (or "multiple testing") problem occurs when one considers a set, or family, of statistical inferences simultaneously. Errors in inference, including confidence intervals that fail to include their corresponding population parameters, or hypothesis tests that incorrectly reject the null hypothesis, are more likely to occur when one considers the family as a whole. Several statistical techniques have been developed to prevent this from happening, allowing significance levels for single and multiple comparisons to be directly compared. These techniques generally require a stronger level of evidence to be observed in order for an individual comparison to be deemed "significant", so as to compensate for the number of inferences being made.

The term "comparisons" in multiple comparisons typically refers to comparisons of two groups, such as a treatment group and a control group. "Multiple comparisons" arise when a statistical analysis encompases a number of formal comparisons, with the presumption that attention will focus on the strongest differences among all comparisons that are made. Failure to compensate for multiple comparisons can have important real-world consequences, as illustrated by the following examples.
In all three examples, as the number of comparisons increases, it becomes more likely that the groups being compared will appear to differ in terms of at least one attribute. However a difference between the groups is only meaningful if it generalizes to an independent sample of data (e.g. to an independent set of people treated with the same drug). Our confidence that a result will generalize to independent data should generally be weaker if it is observed as part of an analysis that involves multiple comparisons, rather than an analysis that involves only a single comparison.

The family of statistical inferences that occur in a multiple comparisons analysis can comprise confidence intervals, hypothesis tests, or both in combination.
To illustrate the issue in terms of confidence intervals, note that a single confidence interval at the 95% level will likely contain the population parameter it is meant to contain. However, if one considers 100 confidence intervals, simultaneously, with coverage probability 95% each, it is highly likely that at least one will not contain its population parameter. The expected number of such intervals is 5, and if the intervals are independent, the probability that at least one interval does not contain the population parameter is 99.4%.
If the inferences are hypothesis tests rather than confidence intervals, the same issue arises. With just one test, performed at the 5% level, there is only a 5% chance of incorrectly rejecting the null hypothesis. However, with 100 tests where all null hypotheses are true, the expected number of incorrect rejections is 5. If the tests are independent, the probability of at least one incorrect rejection is 99.4%. The situation is analogous to the confidence interval case. These errors are called false positives. Many techniques have been developed to control the false positive error rate associated with making multiple statistical comparisons.

For example, one might declare that a coin was biased if in 10 flips it landed heads at least 9 times. Indeed, if one assumes as a null hypothesis that the coin is fair, then the probability that a fair coin would come up heads at least 9 out of 10 times is (10 + 1) Ã— (1/2)10 = 0.0107. This is relatively unlikely, and under statistical criteria such as p-value < 0.05, one would declare that the null hypothesis should be rejected â€” i.e. the coin is unfair.
A multiple comparisons problem arises if one wanted to use this test (which is appropriate for testing the fairness of a single coin), to test the fairness of many coins. Imagine if one was to test 100 fair coins by this method. Given that the probability of a fair coin coming up 9 or 10 heads in 10 flips is 0.0107, one would expect that in flipping 100 fair coins ten times each, to see a particular (i.e. pre-selected) coin come up heads 9 or 10 times would still be very unlikely, but seeing some coin, it doesn't matter which one, behave that way would be more likely than not. Precisely, the likelihood that all 100 fair coins are identified as fair by this criterion is (1 âˆ’ 0.0107)100 â‰ˆ 0.34. Therefore the application of our single-test coin-fairness criterion to multiple comparisons would more likely than not falsely identify at least one fair coin as unfair.

For hypothesis testing, the problem of multiple comparisons (also known as the multiple testing problem) results from the increase in type I error that occurs when statistical tests are used repeatedly. If n independent comparisons are performed, the experiment-wide significance level Î±, also termed FWER for familywise error rate, is given by
Unless the tests are perfectly dependent, Î± increases as the number of comparisons increases. If we do not assume that the comparisons are independent, then we can still say:
which follows from Boole's inequality.

In order to retain a prescribed familywise error rate Î± in an analysis involving more than one comparison, the error rate for each comparison must be more stringent than Î±. Boole's inequality implies that if each test is performed to have type I error rate Î±/n, the total error rate will not exceed Î±. This is called the Bonferroni correction, and is one of the most commonly used approaches for multiple comparisons.
In some situations, the Bonferroni correction is substantially conservative, i.e., the actual familywise error rate is much less than the prescribed level Î±. This occurs when the test statistics are highly dependent (in the extreme case where the tests are perfectly dependent, the familywise error rate with no multiple comparisons adjustment and the per-test error rates are identical). For example, in fMRI analysis, tests are done on over 100000 voxels in the brain. The Bonferroni method would require p-values to be smaller than .05/100000 to declare significance. Since adjacent voxels tend to be highly correlated, this threshold is generally too stringent.
Because simple techniques such as the Bonferroni method can be too conservative, there has been a great deal of attention paid to developing better techniques, such that the overall rate of false positives can be maintained without inflating the rate of false negatives unnecessarily. Such methods can be divided into general categories:
The advent of computerized resampling methods, such as bootstrapping and Monte Carlo simulations, has given rise to many techniques in the latter category. In some cases where exhaustive permutation resampling is performed, these tests provide exact, strong control of Type I error rates; in other cases, such as bootstrap sampling, they provide only approximate control.

Multiple comparison procedures are commonly used after obtaining a significant omnibus test, like the ANOVA F-test. The significant ANOVA result suggests rejecting the global null hypothesis H0 = "means are the same". Multiple comparison procedures are then used to determine which means are different from which.
Comparing K means involves K(K âˆ’ 1)/2 pairwise comparisons.
The Kruskalâ€“Wallis test is the non-parametric alternative to ANOVA. Multiple comparisons can be done using pairwise comparisons (for example using Wilcoxon rank sum tests) and using a correction to determine if the post-hoc tests are significiant (for example a Bonferroni correction).

Traditionally, work on multiple comparisons focused on correcting for modest numbers of comparisons, often in an analysis of variance. More recently, focus has shifted to "large-scale multiple testing" in which thousands or even greater numbers of tests are performed. For example, in genomics, when using technologies such as microarrays, expression levels of tens of thousands of genes can be measured, and genotypes for millions of genes can be measured. While methods to control the familywise error rate are used in these problems, one can alternatively control the false discovery rate (FDR), defined to be the expected proportion of false positives among all significant tests. One simple meta-test is to use a Poisson distribution whose mean is the expected number of significant tests, equal to Î± times the number of comparisons, to estimate the likelihood of finding any given number of significant tests.


