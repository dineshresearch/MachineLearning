Overclocking
Overclocking is the process of running a computer component at a higher clock rate (more clock cycles per second) than it was designed for or was specified by the manufacturer, usually practiced by personal computer enthusiasts seeking an increase in the performance of their computers. Some of them purchase low-end computer components which they then overclock to higher speeds, or overclock high-end components to attain levels of performance beyond the specified values. Others overclock outdated components to keep pace with new system requirements, rather than purchasing new hardware.[1]
Users who overclock their components mainly focus their efforts on processors, video cards, motherboard chipsets, and Random Access Memory (RAM). It is done through manipulating the CPU multiplier and the motherboard's front side bus (FSB) speed until a maximum stable operating frequency is reached, although with the introduction of Intel's new X58 chipset and the Core i7 Processor, the Front Side Bus has been replaced with the QPI (Quick Path Interconnect); often this is called the Baseclock (BCLK). While the idea is simple, variation in the electrical and physical characteristics of computing systems complicates the process. CPU multipliers, bus dividers, voltages, thermal loads, cooling techniques and several other factors can affect it.[2]


There are several considerations when overclocking. First is to ensure that the component is supplied with adequate power to operate at the new speed. However, supplying the power with improper settings or applying excessive voltage can permanently damage a component. Since tight tolerances are required for overclocking, only more expensive motherboardsâ€”with advanced settings that computer enthusiasts are likely to useâ€”have built-in overclocking capabilities. Motherboards with fewer settings, such as those found in Original Equipment Manufacturer (OEM) systems, do not support overclocking.

All electronic circuits produce heat generated by the movement of electrons. As clock frequencies in digital circuits and voltage applied increase, the heat generated by components running at the clock speed also increases. Because of increased heat produced by overclocked components, effective cooling is necessary to avoid damaging the hardware. In addition, some digital circuits slow down at high temperatures due to changes in metalâ€“oxideâ€“semiconductor field-effect transistor (MOSFET) device characteristics. Wire resistance also increases slightly at higher temperatures, contributing to decreased circuit performance[citation needed].
Because most stock cooling systems are designed for the amount of power produced during non-overclocked use, overclockers typically turn to more effective cooling solutions, such as powerful fans, larger heatsinks, heat pipes and water cooling. Size, shape, and material all influence the ability of a heatsink to dissipate heat. Efficient heatsinks are often made entirely of copper, which has high thermal conductivity, but is expensive.[3] Aluminium is more widely used; it has poorer thermal conductivity, but is significantly cheaper than copper. Heat pipes are commonly used to improve conductivity. Many heatsinks combine two or more materials to achieve a balance between performance and cost.[3]
Water cooling carries waste heat to a radiator. Thermoelectric cooling devices, also known as Peltier devices, are recently popular with the onset of high Thermal Design Power (TDP) processors made by Intel and AMD. Thermoelectric cooling devices create temperature differences between two plates by running an electric current through the plates. This method of cooling is highly effective, but itself generates significant heat. For this reason, it is often necessary to supplement thermoelectric cooling devices with a convection-based heatsink or a water-cooling system.
Other cooling methods are forced convection and phase change cooling which is used in refrigerators. Liquid nitrogen, liquid helium, and dry ice are used as coolants in extreme cases,[4] such as record-setting attempts or one-off experiments rather than cooling an everyday system. In June 2006, IBM and Georgia Institute of Technology jointly announced a new record in silicon-based chip speed above 500 GHz, which was done by cooling the chip to 4.5 K (âˆ’268.7 Â°C; âˆ’451.6 Â°F) using liquid helium.[5] These extreme methods are generally impractical in the long term, as they require refilling reservoirs of vaporizing coolant, and condensation can be formed on chilled components.[4] Moreover, silicon-based junction gate field-effect transistors (JFET) will degrade below temperatures of roughly 100 K (âˆ’173 Â°C; âˆ’280 Â°F) and eventually cease to function or "freeze out" at 40 K (âˆ’233 Â°C; âˆ’388 Â°F) since the silicon ceases to be semiconducting[6] so using extremely cold coolants may cause devices to fail.
Submersion cooling, used by the Cray-2 supercomputer, involves sinking a part of computer system directly into a chilled liquid that is thermally conductive but has low electrical conductivity. The advantage of this technique is that no condensation can form on components.[7] A good submersion liquid is Fluorinert made by 3M, which is expensive and can only be purchased with a permit.[7] Another option is mineral oil, but impurities such as those in water might cause it to conduct electricity.[7]

As an overclocked component operates outside of the manufacturer's recommended operating conditions, it may function incorrectly, leading to system instability. Another risk is silent data corruption by undetected errors. Such failures might never be correctly diagnosed and may instead be incorrectly attributed to software bugs in applications or the operating system. Overclocked use may permanently damage components enough to cause them to misbehave (even under normal operating conditions) without becoming totally unusable.
In general, overclockers claim that testing can ensure that an overclocked system is stable and functioning correctly. Although software tools are available for testing hardware stability, it is generally impossible for any private individual to thoroughly test the functionality of a processor. Achieving good fault coverage requires immense engineering effort; even with all of the resources dedicated to validation by manufacturers, faulty components and even design faults are not always detected.
A particular "stress test" can verify only the functionality of the specific instruction sequence used in combination with the data and may not detect faults in those operations. For example, an arithmetic operation may produce the correct result but incorrect flags; if the flags are not checked, the error will go undetected.
To further complicate matters, in process technologies such as silicon on insulator, devices display hysteresisâ€”a circuit's performance is affected by the events of the past, so without carefully targeted tests it is possible for a particular sequence of state changes to work at overclocked speeds in one situation but not another even if the voltage and temperature are the same. Often, an overclocked system which passes stress tests experiences instabilities in other programs.[8]
In overclocking circles, "stress tests" or "torture tests" are used to check for correct operation of a component. These workloads are selected as they put a very high load on the component of interest (e.g. a graphically-intensive application for testing video cards, or different math-intensive applications for testing general CPUs). Popular stress tests include Prime95, OCCT, IntelBurnTest/Linpack/LinX, SiSoftware Sandra, BOINC, Intel Thermal Analysis Tool and Memtest86. The hope is that any functional-correctness issues with the overclocked component will show up during these tests, and if no errors are detected during the test, the component is then deemed "stable". Since fault coverage is important in stability testing, the tests are often run for long periods of time, hours or even days.

Overclockability arises in part due to the economics of the manufacturing processes of CPUs and other components. In most cases components with different rated clock speeds are manufactured by the same process, and tested after manufacture to determine their actual ratings. The clock speed that the component is rated for is at or below the speed at which the CPU has passed the manufacturer's functionality tests when operating in worst-case conditions (for example, the highest allowed temperature and lowest allowed supply voltage). Manufacturers must also leave additional margin for reasons discussed below. Sometimes manufacturers produce more high-performing parts than they can sell, so some are marked as medium-speed chips to be sold for medium prices. The performance of a given CPU stepping usually does not vary as widely as the marketing clock levels[citation needed].

Benchmarks are used to evaluate performance. The benchmarks can themselves become a kind of 'sport', in which users compete for the highest scores. As discussed above, stability and functional correctness may be compromised when overclocking, and meaningful benchmark results depend on correct execution of the benchmark. Because of this, benchmark scores may be qualified with stability and correctness notes (e.g. an overclocker may report a score, noting that the benchmark only runs to completion 1 in 5 times, or that signs of incorrect execution such as display corruption are visible while running the benchmark).
Given only benchmark scores it may be difficult to judge the difference overclocking makes to the overall speed of a computer. For example, some benchmarks test only one aspect of the system, such as memory bandwidth, without taking into consideration how higher speeds in this aspect will improve the system performance as a whole. Apart from demanding applications such as video encoding, high-demand databases and scientific computing, memory bandwidth is typically not a bottleneck, so a great increase in memory bandwidth may be unnoticeable to a user depending on the applications used. Other benchmarks, such as 3DMark attempt to replicate game conditions.

The extent to which a particular part will overclock is highly variable. Processors from different vendors, production batches, steppings, and individual units will all overclock with varying degrees of success.

Commercial system builders or component resellers sometimes overclock to sell items at higher profit margins. The retailer makes more money by buying lower-value components, overclocking them, and selling them at prices appropriate to a non-overclocked system at the new speed. In some cases an overclocked component is functionally identical to a non-overclocked one of the new speed, however, if an overclocked system is marketed as a non-overclocked system (it is generally assumed that unless a system is specifically marked as overclocked, it is not overclocked), it is considered fraudulent.
Overclocking is sometimes offered as a legitimate service or feature for consumers, in which a manufacturer or retailer tests the overclocking capability of processors, memory, video cards, and other hardware products. Several video card manufactures now offer factory overclocked versions of their graphics accelerators, complete with a warranty, which offers an attractive solution for enthusiasts seeking an improved performance without sacrificing common warranty protections. Such factory-overclocked products may cost a little more than standard components, but may be more cost-effective than product with a higher specification.
Naturally, manufacturers would prefer enthusiasts to pay additional money for profitable high-end products, in addition to concerns of less reliable components and shortened product life spans affecting brand image. It is speculated that such concerns are often motivating factors for manufacturers to implement overclocking prevention mechanisms such as CPU locking. These measures are sometimes marketed as a consumer protection benefit, which typically generates a negative reception from overclocking enthusiasts.


Many of the disadvantages of overclocking can be mitigated or reduced in severity by skilled overclockers. However, novice overclockers may make mistakes while overclocking which can introduce avoidable drawbacks and which are more likely to damage the overclocked components (as well as other components they might affect).

These disadvantages are unavoidable by both novices and veterans.


The utility of overclocking is limited for a few reasons:

Graphics cards can also be overclocked[11], with utilities such as NVIDIA's Coolbits, RivaTuner, or the PEG Link Mode on ASUS motherboards. Overclocking a GPU will often yield a marked increase in performance in synthetic benchmarks, but usually does not offer a similar increase in gaming performance[citation needed] (5-10 FPS is generally considered a good result). Just like overclocking a processor, sufficient cooling is a must.
Sometimes, it is possible to see that a graphics card is pushed beyond its limits before any permanent damage is done by observing on-screen distortions known as artifacts. Two such discriminated "warning bells" are widely understood: green-flashing, random triangles appearing on the screen usually correspond to overheating problems on the GPU itself, while white, flashing dots appearing randomly (usually in groups) on the screen often mean that the card's RAM is overheating. It is common to run into one of those problems when overclocking graphics cards. Showing both symptoms at the same time usually means that the card is severely pushed beyond its heat/speed/voltage limits. If seen at normal speed, voltage and temperatur
